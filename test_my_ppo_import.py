#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æœ¬åœ°å®‰è£…çš„ rsl_rl åŒ…æ˜¯å¦æ­£ç¡®
åŒ…æ‹¬ MY_PPO, Encoder, ForceEstimator, My_RolloutStorage ç­‰æ¨¡å—
"""

import sys
import torch

print("="*80)
print("æµ‹è¯• rsl_rl æœ¬åœ°å®‰è£…")
print("="*80)

# æµ‹è¯• 1: å¯¼å…¥åŸºç¡€ç®—æ³•ç±»
print("\n[æµ‹è¯• 1] å¯¼å…¥ç®—æ³•ç±»...")
try:
    from rsl_rl.algorithms import PPO, Distillation, MY_PPO
    print("âœ… æˆåŠŸå¯¼å…¥: PPO, Distillation, MY_PPO")
    print(f"   MY_PPO ç±»ä½ç½®: {MY_PPO.__module__}")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 2: å¯¼å…¥ç½‘ç»œæ¨¡å—
print("\n[æµ‹è¯• 2] å¯¼å…¥ç½‘ç»œæ¨¡å—...")
try:
    from rsl_rl.modules import ActorCritic, Encoder, ForceEstimator
    print("âœ… æˆåŠŸå¯¼å…¥: ActorCritic, Encoder, ForceEstimator")
    print(f"   Encoder ç±»ä½ç½®: {Encoder.__module__}")
    print(f"   ForceEstimator ç±»ä½ç½®: {ForceEstimator.__module__}")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 3: å¯¼å…¥å­˜å‚¨ç±»
print("\n[æµ‹è¯• 3] å¯¼å…¥å­˜å‚¨ç±»...")
try:
    from rsl_rl.storage import My_RolloutStorage
    print("âœ… æˆåŠŸå¯¼å…¥: My_RolloutStorage")
    print(f"   My_RolloutStorage ç±»ä½ç½®: {My_RolloutStorage.__module__}")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 4: å¯¼å…¥ Runner
print("\n[æµ‹è¯• 4] å¯¼å…¥ Runner...")
try:
    from rsl_rl.runners import OnPolicyRunner1
    print("âœ… æˆåŠŸå¯¼å…¥: OnPolicyRunner1")
    print(f"   OnPolicyRunner1 ç±»ä½ç½®: {OnPolicyRunner1.__module__}")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 5: å®ä¾‹åŒ– Encoder
print("\n[æµ‹è¯• 5] å®ä¾‹åŒ– Encoder...")
try:
    encoder = Encoder(num_obs=50, feature_dim=128)
    print("âœ… Encoder å®ä¾‹åŒ–æˆåŠŸ")
    print(f"   is_recurrent: {encoder.is_recurrent}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_obs = torch.randn(4, 50)
    features = encoder(test_obs)
    print(f"   è¾“å…¥å½¢çŠ¶: {test_obs.shape} â†’ è¾“å‡ºå½¢çŠ¶: {features.shape}")
    assert features.shape == (4, 128), "ç‰¹å¾ç»´åº¦ä¸æ­£ç¡®"
    print("âœ… Encoder å‰å‘ä¼ æ’­æ­£ç¡®")
except Exception as e:
    print(f"âŒ Encoder æµ‹è¯•å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 6: å®ä¾‹åŒ– ForceEstimator
print("\n[æµ‹è¯• 6] å®ä¾‹åŒ– ForceEstimator...")
try:
    force_estimator = ForceEstimator(feature_dim=128, num_force=9)
    print("âœ… ForceEstimator å®ä¾‹åŒ–æˆåŠŸ")
    print(f"   is_recurrent: {force_estimator.is_recurrent}")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_features = torch.randn(4, 128)
    estimated_forces = force_estimator(test_features)
    print(f"   è¾“å…¥å½¢çŠ¶: {test_features.shape} â†’ è¾“å‡ºå½¢çŠ¶: {estimated_forces.shape}")
    assert estimated_forces.shape == (4, 27), "åŠ›ä¼°è®¡ç»´åº¦ä¸æ­£ç¡® (åº”è¯¥æ˜¯ 9*3=27)"
    print("âœ… ForceEstimator å‰å‘ä¼ æ’­æ­£ç¡®")
except Exception as e:
    print(f"âŒ ForceEstimator æµ‹è¯•å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 7: å®ä¾‹åŒ– ActorCritic
print("\n[æµ‹è¯• 7] å®ä¾‹åŒ– ActorCritic...")
try:
    policy = ActorCritic(
        num_actor_obs=128,  # ä½¿ç”¨ feature_dim
        num_critic_obs=128,
        num_actions=12
    )
    print("âœ… ActorCritic å®ä¾‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_features = torch.randn(4, 128)
    actions = policy.act(test_features)
    values = policy.evaluate(test_features)
    print(f"   ç‰¹å¾å½¢çŠ¶: {test_features.shape} â†’ åŠ¨ä½œå½¢çŠ¶: {actions.shape}, å€¼å½¢çŠ¶: {values.shape}")
    assert actions.shape == (4, 12), "åŠ¨ä½œç»´åº¦ä¸æ­£ç¡®"
    assert values.shape == (4, 1), "å€¼ç»´åº¦ä¸æ­£ç¡®"
    print("âœ… ActorCritic å‰å‘ä¼ æ’­æ­£ç¡®")
except Exception as e:
    print(f"âŒ ActorCritic æµ‹è¯•å¤±è´¥: {e}")
    sys.exit(1)

# æµ‹è¯• 8: å®ä¾‹åŒ– MY_PPO
print("\n[æµ‹è¯• 8] å®ä¾‹åŒ– MY_PPO...")
try:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"   ä½¿ç”¨è®¾å¤‡: {device}")
    
    policy = ActorCritic(
        num_actor_obs=128,
        num_critic_obs=128,
        num_actions=12
    ).to(device)
    
    alg = MY_PPO(
        policy=policy,
        device=device,
        feature_dim=128,
        raw_obs_dim=50,  # åŸå§‹è§‚æµ‹ç»´åº¦
        num_learning_epochs=5,
        num_mini_batches=4,
        learning_rate=1e-3
    )
    print("âœ… MY_PPO å®ä¾‹åŒ–æˆåŠŸ")
    print(f"   Encoder: {alg.encoder}")
    print(f"   ForceEstimator: {alg.force_estimator}")
    print(f"   ä¼˜åŒ–å™¨åŒ…å«å‚æ•°ç»„æ•°: {len(list(alg.optimizer.param_groups))}")
except Exception as e:
    print(f"âŒ MY_PPO æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯• 9: åˆå§‹åŒ–å­˜å‚¨
print("\n[æµ‹è¯• 9] åˆå§‹åŒ– RolloutStorage...")
try:
    alg.init_storage(
        training_type="rl",
        num_envs=10,
        num_transitions_per_env=24,
        actor_obs_shape=[50],
        critic_obs_shape=[50],
        actions_shape=[12]
    )
    print("âœ… RolloutStorage åˆå§‹åŒ–æˆåŠŸ")
    print(f"   å­˜å‚¨ç±»å‹: {type(alg.storage)}")
    print(f"   latent_feature å½¢çŠ¶: {alg.storage.latent_feature.shape}")
    print(f"   real_forces å½¢çŠ¶: {alg.storage.real_forces.shape}")
except Exception as e:
    print(f"âŒ RolloutStorage æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯• 10: æµ‹è¯• act æ–¹æ³•
print("\n[æµ‹è¯• 10] æµ‹è¯• MY_PPO.act() æ–¹æ³•...")
try:
    obs = torch.randn(10, 50).to(device)
    critic_obs = torch.randn(10, 50).to(device)
    
    actions = alg.act(obs, critic_obs)
    print("âœ… act() æ–¹æ³•æ‰§è¡ŒæˆåŠŸ")
    print(f"   è§‚æµ‹å½¢çŠ¶: {obs.shape}")
    print(f"   åŠ¨ä½œå½¢çŠ¶: {actions.shape}")
    print(f"   latent_feature å½¢çŠ¶: {alg.transition.latent_feature.shape}")
    assert actions.shape == (10, 12), "åŠ¨ä½œç»´åº¦ä¸æ­£ç¡®"
    assert alg.transition.latent_feature.shape == (10, 128), "latent_feature ç»´åº¦ä¸æ­£ç¡®"
    print("âœ… act() è¾“å‡ºç»´åº¦æ­£ç¡®")
except Exception as e:
    print(f"âŒ act() æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯• 11: æµ‹è¯• process_env_step æ–¹æ³•
print("\n[æµ‹è¯• 11] æµ‹è¯• MY_PPO.process_env_step() æ–¹æ³•...")
try:
    rewards = torch.randn(10).to(device)
    dones = torch.zeros(10).to(device)
    real_forces = torch.randn(10, 27).to(device)  # 9 links * 3
    infos = {"real_forces": real_forces}
    
    alg.process_env_step(rewards, dones, infos)
    print("âœ… process_env_step() æ–¹æ³•æ‰§è¡ŒæˆåŠŸ")
    print(f"   å­˜å‚¨çš„ real_forces å½¢çŠ¶: {alg.storage.real_forces[alg.storage.step-1].shape}")
except Exception as e:
    print(f"âŒ process_env_step() æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# æµ‹è¯• 12: æ£€æŸ¥ä¼˜åŒ–å™¨å‚æ•°
print("\n[æµ‹è¯• 12] æ£€æŸ¥ä¼˜åŒ–å™¨åŒ…å«çš„å‚æ•°...")
try:
    total_params = 0
    encoder_params = sum(p.numel() for p in alg.encoder.parameters())
    force_estimator_params = sum(p.numel() for p in alg.force_estimator.parameters())
    policy_params = sum(p.numel() for p in alg.policy.parameters())
    
    print(f"   Encoder å‚æ•°é‡: {encoder_params:,}")
    print(f"   ForceEstimator å‚æ•°é‡: {force_estimator_params:,}")
    print(f"   Policy å‚æ•°é‡: {policy_params:,}")
    print(f"   æ€»å‚æ•°é‡: {encoder_params + force_estimator_params + policy_params:,}")
    
    # éªŒè¯ä¼˜åŒ–å™¨åŒ…å«æ‰€æœ‰å‚æ•°
    optimizer_params = sum(p.numel() for group in alg.optimizer.param_groups for p in group['params'])
    print(f"   ä¼˜åŒ–å™¨ç®¡ç†çš„å‚æ•°é‡: {optimizer_params:,}")
    
    if optimizer_params == encoder_params + force_estimator_params + policy_params:
        print("âœ… ä¼˜åŒ–å™¨æ­£ç¡®åŒ…å«æ‰€æœ‰ç½‘ç»œçš„å‚æ•°")
    else:
        print("âš ï¸ è­¦å‘Š: ä¼˜åŒ–å™¨å‚æ•°é‡ä¸åŒ¹é…")
except Exception as e:
    print(f"âŒ å‚æ•°æ£€æŸ¥å¤±è´¥: {e}")
    sys.exit(1)

print("\n" + "="*80)
print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼rsl_rl æœ¬åœ°å®‰è£…æ­£ç¡®ï¼ŒMY_PPO åŠç›¸å…³æ¨¡å—å·¥ä½œæ­£å¸¸ï¼")
print("="*80)

